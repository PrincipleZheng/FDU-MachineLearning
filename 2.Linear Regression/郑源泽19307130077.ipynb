{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Machine Learning: Assignment 2</center>\n",
    "**<center>郑源泽  19307130077</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务描述\n",
    "设计一个回归系统来预测住房价格。\n",
    "\n",
    "数据集：https://www.kaggle.com/vikrishnan/boston-house-prices（也可在Sklearn上找到）。\n",
    "\n",
    "回归算法应该包含 ridge regression 和 lasso regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集描述\n",
    "数据集来自于1970年的波士顿标准都市统计区（SMSA）。\n",
    "其各项数据的详细意义在kaggle数据集的描述中有详细说明，这里不再赘述。\n",
    "我们需要预测的是房屋的价格，即MEDV。\n",
    "\n",
    "数据集的大概情况如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:(506, 13)\n",
      "target shape:(506,)\n",
      "dataset size:506\n",
      "vector size:13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import sklearn.preprocessing as pre\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, Ridge, LassoCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "boston = load_boston()\n",
    "x, y = boston.data, boston.target\n",
    "print(\"data shape:\"+str(x.shape))\n",
    "print(\"target shape:\"+str(y.shape))\n",
    "print(\"dataset size:\"+str(len(x)))\n",
    "print(\"vector size:\"+str(len(x[0])))\n",
    "# print(type(x[0][0]), type(y[0]))\n",
    "# print(x[0])\n",
    "# print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法描述\n",
    "常见的三种线性回归\n",
    "1. simple linear regression\n",
    "2. ridge regression\n",
    "3. lasso regression\n",
    "\n",
    "进⾏线性回归的计算其实有两种⽅法：最⼩⼆乘法和梯度下降法。本次实验调⽤sklearn库的线性模型，通过查阅⽂档，他们其实都是基于最⼩⼆乘法来实现的。因此模型描述部分介绍最⼩⼆乘法求解线性回归的原理。\n",
    "\n",
    "令$x_j^{(i)}$表示数据集第$i$个数据的第$j$个属性取值，$y^{(i)}$表示数据集第$i$个数据的标签值，$z^{(i)}$表示第$i$组数据的计算预测值。数据集一共有$m$个数据，$n$种属性。\n",
    "\n",
    "矩阵形式表示，$x_i=\\begin{bmatrix}1 & x_1^{(i)} & \\cdots & x_n^{(i)}\\end{bmatrix}$，$X=\\begin{bmatrix}1 & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\ 1 & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ 1 & x_1^{(m)} & \\cdots & x_n^{(m)}\\end{bmatrix}$，参数$W=\\begin{bmatrix}w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n\\end{bmatrix}$，数据集标签为$y=\\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)}\\end{bmatrix}$\n",
    "\n",
    "模型定义为$f(x)=\\sum_{i=0}^n{w_ix_i}$，其中$x_0=1$，矩阵表示为$f(x)=XW$。我们的目的就是寻找最合适的$W$来最好效果地拟合和预测。\n",
    "\n",
    "线性回归试图学得$z = \\sum_{i=0}^nw_ix_i$，使得$z\\simeq y$。而回归任务中常用的学习$W$手段便是均方差(MSE-mean squared error)：\n",
    "$$J=\\frac{1}{2m}\\sum_{i=1}^m(z_i-y_i)^2=\\frac{1}{2m}\\sum_{i=1}^m(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})^2$$\n",
    "我们要让均方差最小，也就要使得$\\frac{\\partial J}{\\partial w}$为0。以$w_k$为例\n",
    "$$\\frac{\\partial J}{\\partial w_k}=\\frac{\\partial( \\frac{1}{2m}\\sum_{i=1}^m(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})^2)}{\\partial w_k}=\\frac{1}{m}\\sum_{i=1}^m[(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})(-x_k^{(i)})]=0$$\n",
    "\n",
    "由此可以得到一个关于$w_k$的方程组，可由此解出各个$w_k$的值，方程组如下:\n",
    "$$\\begin{cases}\n",
    "\\sum_{i=1}^m[(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})(-x_0^{(i)})]=0\\\\\n",
    "\\sum_{i=1}^m[(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})(-x_1^{(i)})]=0\\\\\n",
    "\\cdots\\\\\n",
    "\\sum_{i=1}^m[(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})(-x_n^{(i)})]=0\n",
    "\\end{cases}$$\n",
    "化简如下:\n",
    "$$\\begin{cases}\n",
    "\\sum_{j=0}^n[w_jx_0^{(i)}(\\sum_{i=1}^mx_j^{(i)})]=\\sum_{i=1}^mx_0^{(i)}y^{(i)}\\\\\n",
    "\\sum_{j=0}^n[w_jx_1^{(i)}(\\sum_{i=1}^mx_j^{(i)})]=\\sum_{i=1}^mx_1^{(i)}y^{(i)}\\\\\n",
    "\\cdots\\\\\n",
    "\\sum_{j=0}^n[w_jx_n^{(i)}(\\sum_{i=1}^mx_j^{(i)})]=\\sum_{i=1}^mx_n^{(i)}y^{(i)}\n",
    "\\end{cases}$$\n",
    "\n",
    "我们要求解向量$W=\\begin{bmatrix}w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n\\end{bmatrix}$。上方程组用矩阵形式表示如下\n",
    "\n",
    "$$AW=B \\\\ A^{T}AW=A^{T}B \\\\ W=(A^TA)^{-1}A^TB$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RidgeRegression(岭回归)\n",
    "\n",
    "为最小二乘法估计中加入一个扰动$\\lambda I$，使得原先无法求出广义逆的情况变成可以求出其广义逆，使得问题稳定并得以求解。同时，岭回归在计算`loss`时加入$L2$正则化，即:\n",
    "$$J = \\frac{1}{2m}\\sum_{i=1}^m(z^{(i)}-y^{(i)})^2 + \\lambda\\sum_{i=0}^nw_i^2$$\n",
    "\n",
    "LassoRegression(Lasso回归)\n",
    "\n",
    "Lasso回归与岭回归类似，同样引入扰动$\\lambda I$，但在计算`loss`时加上$L1$正则化，也即:\n",
    "$$J = \\frac{1}{2m}\\sum_{i=1}^m(z^{(i)}-y^{(i)})^2 + \\lambda\\sum_{i=0}^n|w_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "boston = load_boston()\n",
    "\n",
    "x, y = boston.data, boston.target\n",
    "# x = pre.StandardScaler().fit_transform(x)\n",
    "x = pre.MinMaxScaler().fit_transform(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1930)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从两个维度来判断模型拟合的好坏\n",
    "1. 拟合函数的R2,⽤model.score函数来获取，体现拟合曲线对于样本点的拟合\n",
    "2. 预测房价值和真实房价值的差的绝对值平均，⽤mean_absolute_percentage_error函数获得，可以体现预测的是否准确"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 简单线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_linear_regression(x_train, x_test, y_train, y_test):\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    # print(\"mean_absolute_percentage_error:\", round(mean_absolute_percentage_error(y_test, y_pred),4))\n",
    "    # print(\"score:\", round(model.score(x_train, y_train),4))\n",
    "    # print(\"\\n\")\n",
    "    return round(model.score(x_train, y_train),4), round(mean_absolute_percentage_error(y_test, y_pred),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(x_train, x_test, y_train, y_test, my_alpha = 0.1):\n",
    "    model = RidgeCV(alphas = [0.000001, 0.000003, 0.00001, 0.00003, 0.0001, 0.0003,\n",
    "                            0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100])\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(\"最佳alpha是：\", model.alpha_)\n",
    "    # print(\"这是ridge_regression的测试结果：\")\n",
    "    # print(\"mean_absolute_percentage_error:\", round(mean_absolute_percentage_error(y_test, y_pred), 4))\n",
    "    # print(\"score:\", round(model.score(x_train, y_train), 4))\n",
    "    # print(\"\\n\")\n",
    "    return round(model.score(x_train, y_train),4), round(mean_absolute_percentage_error(y_test, y_pred),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression(x_train, x_test, y_train, y_test, my_alpha = 0.1):\n",
    "    model = LassoCV(alphas = [0.0000000000001, 0.0000000000003, 0.0000001, 0.0000003, 0.000001, 0.000003,\n",
    "                              0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1,\n",
    "                              0.3, 1, 3, 10, 30, 100])\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(\"最佳alpha是：\", model.alpha_)\n",
    "    #print(\"这是lasso_regression的测试结果：\")\n",
    "    #print(\"mean_absolute_percentage_error:\", round(mean_absolute_percentage_error(y_test, y_pred), 4))\n",
    "    #print(\"score:\", round(model.score(x_train, y_train), 4))\n",
    "    #print(\"\\n\")\n",
    "    return round(model.score(x_train, y_train),4), round(mean_absolute_percentage_error(y_test, y_pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳alpha是： 1e-13\n",
      "最佳alpha是： 0.003\n",
      "最佳alpha是： 3e-13\n",
      "最佳alpha是： 0.003\n",
      "最佳alpha是： 1e-13\n",
      "最佳alpha是： 1e-13\n",
      "最佳alpha是： 0.003\n",
      "最佳alpha是： 0.003\n",
      "最佳alpha是： 1e-13\n",
      "最佳alpha是： 1e-13\n",
      "average mean_absolute_percentage_error: 0.17192999999999997\n",
      "average score: 0.7415099999999999\n"
     ]
    }
   ],
   "source": [
    "err = 0\n",
    "score = 0\n",
    "for i in range(1024, 1034):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=i+5)\n",
    "    # t1, t2 = simple_linear_regression(x_train, x_test, y_train, y_test)\n",
    "    # t1, t2 = ridge_regression(x_train, x_test, y_train, y_test)\n",
    "    t1, t2 = lasso_regression(x_train, x_test, y_train, y_test)\n",
    "    err = err + t2\n",
    "    score = score + t1\n",
    "print(\"average mean_absolute_percentage_error:\", err/10)\n",
    "print(\"average score:\", score/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验结果\n",
    "- 简单线性回归：0.7415\n",
    "- Ridge回归：0.7411\n",
    "- Lasso回归：0.7415\n",
    "  \n",
    "可能是数据集较规范，没有出现过拟合现象。岭回归和Lasso回归的结果与最简单的线性回归score结果相差不大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "在本次实验中学习了线性回归的数学原理（最小二乘法），并利用一些现有的机器学习库与波士顿房价数据集进行了三种线性回归的手动实践。在实验中加深了对线性回归算法的理解，同时也对机器学习库的使用有了更深的认识。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16f5b46f222e2a3e8d4adbf7141cae37b71ed37616e60735fa5d1164a1bc3ada"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
