{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Machine Learning: Assignment 2</center>\n",
    "**<center>黄绵秋  19307130142</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务描述\n",
    "- 实现线性回归算法\n",
    "- 使用线性回归对iris数据集进行预测\n",
    "\n",
    "## 数据描述\n",
    "- iris数据集一共包含150个样本数据，按照iris种类分成3类，分别被标注为1,2,3，每个样本含有4个特征。\n",
    "- 将特征数据和标签数据分别从sklearn.dataset中导出到变量X,y中\n",
    "- iris数据集的一些信息如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 150\n",
      "feature_names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "target_names: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print('size:', len(X))\n",
    "print('feature_names:', iris.feature_names)\n",
    "print('target_names:', iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 数据处理\n",
    "- 不同特征的数值差别较大，这会导致数量级较大的属性占据主导地位，为防止数量级大的属性的少量变化引起收敛时的震荡和梯度爆炸，应对数据进行标准化处理。鉴于iris数据集较小较精确，且异常点和噪点极少，故采用归一化方式来保证各个特征相等的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[[1.         0.22222222 0.625      0.06779661 0.04166667]\n",
      " [1.         0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [1.         0.11111111 0.5        0.05084746 0.04166667]\n",
      " [1.         0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [1.         0.19444444 0.66666667 0.06779661 0.04166667]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:5])\n",
    "import sklearn.preprocessing\n",
    "X = sklearn.preprocessing.MinMaxScaler().fit_transform(X)\n",
    "X = np.pad(X, ((0,0),(1,0)), 'constant', constant_values=(1))       #为X填充x_0=1\n",
    "# x_0为常值1，用来与w_0相乘，表示偏移量bias\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为对回归结果进行有效评估，将数据分割成训练集和测试集两部分，其中70%为训练集，30%为测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.47222222 0.29166667 0.69491525 0.625     ]\n",
      " [1.         0.44444444 0.41666667 0.69491525 0.70833333]\n",
      " [1.         0.55555556 0.58333333 0.77966102 0.95833333]]\n",
      "数据集size:\n",
      " 训练集 (105, 5) (105,) \n",
      " 测试集 (45, 5) \t (45,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2021, shuffle=True)\n",
    "print(X_train[:3])\n",
    "print('数据集size:\\n', '训练集', X_train.shape, y_train.shape, '\\n 测试集', X_test.shape,'\\t', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归模型\n",
    "\n",
    "### 线性回归概述\n",
    "线性回归用于在监督学习中预测输入变量和输出变量之间的关系，输入变量的值变化时，预测结果也通过线性回归进行相应变化。回归模型正是表示从输入变量到输出变量之间映射的函数。\n",
    "\n",
    "### 算法推导\n",
    "#### 符号规定\n",
    "$x_j^{(i)}$表示数据集第$i$个数据的第$j$个属性取值，$y^{(i)}$表示数据集第$i$个数据的标签值，$z^{(i)}$表示第$i$组数据的计算预测值。数据集一共有$m$个数据，$n$种属性。\n",
    "\n",
    "矩阵形式表示，$x_i=\\begin{bmatrix}1 & x_1^{(i)} & \\cdots & x_n^{(i)}\\end{bmatrix}$，$X=\\begin{bmatrix}1 & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\ 1 & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ 1 & x_1^{(m)} & \\cdots & x_n^{(m)}\\end{bmatrix}$，参数$W=\\begin{bmatrix}w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n\\end{bmatrix}$，数据集标签为$y=\\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)}\\end{bmatrix}$\n",
    "\n",
    "#### 模型目标\n",
    "模型定义为$f(x)=\\sum_{i=0}^n{w_ix_i}$，其中$x_0=1$，矩阵表示为$f(x)=XW$。我们的目的就是寻找最合适的$W$来最好效果地拟合和预测。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 正规方程法\n",
    "线性回归试图学得$z = \\sum_{i=0}^nw_ix_i$，使得$z\\simeq y$。而回归任务中常用的学习$W$手段便是均方差(MSE-mean squared error)：\n",
    "$$J=\\frac{1}{2m}\\sum_{i=1}^m(z_i-y_i)^2=\\frac{1}{2m}\\sum_{i=1}^m(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})^2$$\n",
    "我们要让均方差最小，也就要使得$\\frac{\\partial J}{\\partial w}$为0。以$w_k$为例\n",
    "$$\\frac{\\partial J}{\\partial w_k}=\\frac{\\partial( \\frac{1}{2m}\\sum_{i=1}^m(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})^2)}{\\partial w_k}=\\frac{1}{m}\\sum_{i=1}^m[(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})(-x_k^{(i)})]=0$$\n",
    "\n",
    "由此可以得到一个关于$w_k$的方程组，可由此解出各个$w_k$的值，方程组如下:\n",
    "$$\\begin{cases}\n",
    "\\sum_{i=1}^m[(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})(-x_0^{(i)})]=0\\\\\n",
    "\\sum_{i=1}^m[(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})(-x_1^{(i)})]=0\\\\\n",
    "\\cdots\\\\\n",
    "\\sum_{i=1}^m[(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})(-x_n^{(i)})]=0\n",
    "\\end{cases}$$\n",
    "化简如下:\n",
    "$$\\begin{cases}\n",
    "\\sum_{j=0}^n[w_jx_0^{(i)}(\\sum_{i=1}^mx_j^{(i)})]=\\sum_{i=1}^mx_0^{(i)}y^{(i)}\\\\\n",
    "\\sum_{j=0}^n[w_jx_1^{(i)}(\\sum_{i=1}^mx_j^{(i)})]=\\sum_{i=1}^mx_1^{(i)}y^{(i)}\\\\\n",
    "\\cdots\\\\\n",
    "\\sum_{j=0}^n[w_jx_n^{(i)}(\\sum_{i=1}^mx_j^{(i)})]=\\sum_{i=1}^mx_n^{(i)}y^{(i)}\n",
    "\\end{cases}$$\n",
    "\n",
    "我们要求解的便是向量$W=\\begin{bmatrix}w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n\\end{bmatrix}$。记矩阵$A=\\begin{bmatrix}\\sum_{i=1}^m(x_0^{(i)}x_0^{(i)}) & \\sum_{i=1}^m(x_0^{(i)}x_1^{(i)}) & \\cdots & \\sum_{i=1}^m(x_0^{(i)}x_n^{(i)}) \\\\ \\sum_{i=1}^m(x_1^{(i)}x_0^{(i)}) & \\sum_{i=1}^m(x_1^{(i)}x_1^{(i)}) & \\cdots & \\sum_{i=1}^m(x_1^{(i)}x_n^{(i)})\\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ \\sum_{i=1}^m(x_n^{(i)}x_0^{(i)}) & \\sum_{i=1}^m(x_n^{(i)}x_1^{(i)}) & \\cdots & \\sum_{i=1}^m(x_n^{(i)}x_n^{(i)}) \\end{bmatrix}$，矩阵$B=\\begin{bmatrix}\\sum_{i=1}^m(x_0^{(i)}y^{(i)})\\\\\\sum_{i=1}^m(x_1^{(i)}y^{(i)})\\\\\\cdots \\\\ \\sum_{i=1}^m(x_n^{(i)}y^{(i)})\\end{bmatrix}$。\n",
    "\n",
    "由线代知识易知\n",
    "$$AW=B \\\\ A^{T}AW=A^{T}B \\\\ W=(A^TA)^{-1}A^TB$$\n",
    "这个过程利用Python中解方程的函数即可求得W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降法\n",
    "与正规方程法类似，梯度下降法也是使得MSE尽可能小，但并不通过直接计算，而是根据梯度对数据进行调整，利用导数传递误差，再通过迭代的方式一步一步逼近真实解。\n",
    "\n",
    "损失函数$loss^{(i)}=\\frac{1}{2m}(z^{(i)}-y^{(i)})^2$\n",
    "\n",
    "计算$z$的梯度:$\\frac{\\partial J^{(i)}}{\\partial z^{(i)}}=\\frac{1}{m}(z^{(i)}-y^{(i)})$\n",
    "\n",
    "计算$W$的梯度:\n",
    "$$\\begin{aligned}\\frac{\\partial J}{\\partial W}&=\\begin{bmatrix}\\frac{\\partial J}{\\partial w_0}\\\\ \\frac{\\partial J}{\\partial w_1}\\\\ \\vdots\\\\ \\frac{\\partial J}{\\partial w_n}\\end{bmatrix}=\\begin{bmatrix}\\sum_{i=1}^m(\\frac{\\partial J}{\\partial z_i}\\frac{\\partial z_i}{\\partial w_0})\\\\\\sum_{i=1}^m(\\frac{\\partial J}{\\partial z_i}\\frac{\\partial z_i}{\\partial w_1})\\\\\\vdots\\\\\\sum_{i=1}^m(\\frac{\\partial J}{\\partial z_i}\\frac{\\partial z_i}{\\partial w_n})\\end{bmatrix} \\\\&=\\begin{bmatrix}\\frac{1}{m}\\sum_{i=1}^m(z^{(i)}-y^{(i)})x_0^{(i)}\\\\\\frac{1}{m}\\sum_{i=1}^m(z^{(i)}-y^{(i)})x_1^{(i)}\\\\\\vdots\\\\\\frac{1}{m}\\sum_{i=1}^m(z^{(i)}-y^{(i)})x_n^{(i)}\\end{bmatrix}\\\\&=\\frac{1}{m}\\begin{bmatrix}1 & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\ 1 & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ 1 & x_1^{(m)} & \\cdots & x_n^{(m)}\\end{bmatrix}^T\\begin{bmatrix}z^{(1)}-y^{(1)}\\\\z^{(2)}-y^{(2)}\\\\\\vdots\\\\z^{(m)}-y^{(m)}\\end{bmatrix}\\\\&=\\frac{1}{m}X^T(Z-Y)\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型实现\n",
    "#### 最小二乘法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalEquation():\n",
    "    def __init__(self, feature_dimension):\n",
    "        \"\"\"Initialize variable matrix W\n",
    "\n",
    "        Args:\n",
    "            feature_dimension (int): count of feature\n",
    "        \"\"\"\n",
    "        self.D = feature_dimension\n",
    "        self.W = np.zeros((self.D, ))\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Use normal equation with input X and y to calculate W\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((sample_number, feature_dimension))): matrix X\n",
    "            y (numpy.array((feature_dimension, ))): vector y\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        n = X.shape[1]\n",
    "        A = [[np.sum([X[i][k]*X[i][j] for i in range(m)]) for k in range(n)] for j in range(n)]\n",
    "        A = np.array(A)\n",
    "        B = [np.sum([X[i][j]*y[i] for i in range(m)]) for j in range(n)]\n",
    "        B = np.array(B).T\n",
    "        self.W = np.linalg.solve(A, B)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Use variable matrix W to predict the label result\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((test_sample, feature_dimension))): the input matrix X\n",
    "\n",
    "        Returns:\n",
    "            [int]: the prediction of the label result.\n",
    "        \"\"\"\n",
    "        m = np.array(np.dot(X, self.W))\n",
    "        return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降法实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self, feature_dimension):\n",
    "        \"\"\"Initialize variable matrix W\n",
    "\n",
    "        Args:\n",
    "            feature_dimension (int): count of feature\n",
    "        \"\"\"\n",
    "        self.D = feature_dimension\n",
    "        self.W = np.zeros((self.D, ))\n",
    "        return\n",
    "\n",
    "    def cal_loss(self, X, y):\n",
    "        \"\"\"calculate the loss function using input matrix X and vector y\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((sample_number, feature_dimension))): matrix X\n",
    "            y (numpy.array((feature_dimension, ))): vector y\n",
    "\n",
    "        Returns:\n",
    "            [float]: the loss of the training data\n",
    "            [numpy.array]: delta W\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        w = self.W\n",
    "        y_hat = np.dot(X, w)\n",
    "        dy = y_hat - y\n",
    "        loss = dy.T.dot(dy)\n",
    "        loss = loss / 2 / m\n",
    "        dw = X.T.dot(dy)/m\n",
    "        return loss, dw\n",
    "        \n",
    "    def train(self, X, y, learning_rate=0.99, iteration=1000, loss_print=False):\n",
    "        \"\"\"Use input matrix X and vector y to train the model.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((sample_number, feature_dimension))): matrix X\n",
    "            y (numpy.array((feature_dimension, ))): vector y\n",
    "            learning_rate (float, optional): learning rate(may also called alpha). Defaults to 0.99.\n",
    "            iteration (int, optional): Number of iterations. Defaults to 1000.\n",
    "            loss_print (bool, optional): True if want to print the loss. Defaults to False.\n",
    "        \"\"\"\n",
    "        loss, dw = None, None\n",
    "        for i in range(iteration):\n",
    "            loss, dw = self.cal_loss(X, y)\n",
    "            self.W = self.W - learning_rate * dw\n",
    "            if loss_print:\n",
    "                print('Iteration: %d , Loss: %.2f' % (i, loss))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Use variable matriX W to predict the label result\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((test_sample, feature_dimension))): the input matrix X\n",
    "\n",
    "        Returns:\n",
    "            [int]: the prediction of the label result.\n",
    "        \"\"\"\n",
    "        return X.dot(self.W)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型结果\n",
    "#### 预测正确性判断\n",
    "分别用两种模型进行训练并预测，并取四舍五入的结果作为预测结果，通过统计预测的正确性来判断模型的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal equation test score: 0.9556\n",
      "Linear regression with 10 iterations test score: 0.7333\n",
      "Linear regression with 100 iterations test score: 0.9556\n",
      "Linear regression with 1000 iterations test score: 0.9556\n",
      "Linear regression with 10000 iterations test score: 0.9556\n"
     ]
    }
   ],
   "source": [
    "def score(test_result, y_test):\n",
    "    counter = 0\n",
    "    acc = 0\n",
    "    for (pred_label, true_label) in zip(test_result, y_test):\n",
    "        counter += 1\n",
    "        if pred_label == true_label:\n",
    "            acc += 1\n",
    "    return acc/counter\n",
    "\n",
    "    \n",
    "normal_equation = NormalEquation(X_train.shape[1])\n",
    "normal_equation.train(X_train, y_train)\n",
    "test_result = normal_equation.predict(X_test).round()\n",
    "print('Normal equation test score: %.4f' % score(test_result, y_test))\n",
    "\n",
    "lr = LinearRegression(X_train.shape[1])\n",
    "lr.train(X_train, y_train, iteration=10)\n",
    "test_result = lr.predict(X_test).round()\n",
    "print('Linear regression with 10 iterations test score: %.4f' % score(test_result, y_test))\n",
    "lr.train(X_train, y_train, iteration=100)\n",
    "test_result = lr.predict(X_test).round()\n",
    "print('Linear regression with 100 iterations test score: %.4f' % score(test_result, y_test))\n",
    "lr.train(X_train, y_train, iteration=1000)\n",
    "test_result = lr.predict(X_test).round()\n",
    "print('Linear regression with 1000 iterations test score: %.4f' % score(test_result, y_test))\n",
    "lr.train(X_train, y_train, iteration=10000)\n",
    "test_result = lr.predict(X_test).round()\n",
    "print('Linear regression with 10000 iterations test score: %.4f' % score(test_result, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性回归绘制\n",
    "取一个特征维度，分别用两种模型进行线性回归，并将回归的直线进行可视化处理，以此直观地判断拟合的程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "regression_X = iris.data[:50]\n",
    "regression_X = sklearn.preprocessing.MinMaxScaler().fit_transform(regression_X)\n",
    "regression_X = np.pad(regression_X, ((0,0),(1,0)), 'constant', constant_values=(1))       #为X填充x_0=1\n",
    "regression_y = iris.target[:50]\n",
    "reg_X_train, reg_X_test, reg_y_train, reg_y_test = train_test_split(regression_X, regression_y, test_size=0.3, random_state=2021, shuffle=True)\n",
    "\n",
    "show_X_train = reg_X_train[:,:2]\n",
    "show_X_test = reg_X_test[:,:2]\n",
    "show_X_test_scatter = reg_X_test[:,1]\n",
    "show_y_train = reg_X_train[:,2:3]\n",
    "show_y_test = reg_X_test[:,2:3]\n",
    "show_y_train = show_y_train.reshape((show_y_train.shape[0],))\n",
    "show_y_test = show_y_test.reshape((show_y_test.shape[0],))\n",
    "\n",
    "show_nor_equ = NormalEquation(2)\n",
    "show_nor_equ.train(show_X_train, show_y_train)\n",
    "show_lr_0 = LinearRegression(2)\n",
    "show_lr_0.train(show_X_train, show_y_train, iteration=0)\n",
    "show_lr_10 = LinearRegression(2)\n",
    "show_lr_10.train(show_X_train, show_y_train, iteration=10)\n",
    "show_lr_100 = LinearRegression(2)\n",
    "show_lr_100.train(show_X_train, show_y_train, iteration=100)\n",
    "show_lr_1000 = LinearRegression(2)\n",
    "show_lr_1000.train(show_X_train, show_y_train, iteration=1000)\n",
    "\n",
    "model_dict = {\n",
    "    show_nor_equ: \"NormalEquation\",\n",
    "    show_lr_0: \"LinearRegression: iter-0\",\n",
    "    show_lr_10: \"LinearRegression: iter-10\",\n",
    "    show_lr_100: \"LinearRegression: iter-100\",\n",
    "    show_lr_1000: \"LinearRegression: iter-1000\"\n",
    "}\n",
    "plt.xlabel('x label')\n",
    "plt.ylabel('y label')\n",
    "plt.scatter(show_X_test_scatter, show_y_test)\n",
    "for model, model_str in model_dict.items():\n",
    "    # model.train(show_X_train, show_y_train)\n",
    "    plt.plot(show_X_test_scatter, model.predict(show_X_test), label=model_str)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型优化\n",
    "#### kFold算法交叉验证\n",
    "将数据集k等分进行交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def score_with_kfold(X, y, model, needIter=False, iter=1000):\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=2021)\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "        Model = model(X_train.shape[1])\n",
    "        if needIter == False:\n",
    "            Model.train(X_train, y_train)\n",
    "        else:\n",
    "            Model.train(X_train, y_train, iteration=iter)\n",
    "        test_result = Model.predict(X_test).round()\n",
    "        scores.append(score(test_result, y_test))\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NormalEquation accuracy: 0.9600\n",
      "LinearRegression-iter 0    accuracy: 0.3333\n",
      "LinearRegression-iter 10   accuracy: 0.7933\n",
      "LinearRegression-iter 100  accuracy: 0.9600\n",
      "LinearRegression-iter 1000 accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "X = sklearn.preprocessing.MinMaxScaler().fit_transform(X)\n",
    "X = np.pad(X, ((0,0),(1,0)), 'constant', constant_values=(1))       #为X填充x_0=1\n",
    "model_nor_equ = NormalEquation\n",
    "iter_list = [0, 10, 100, 1000]\n",
    "print(\"NormalEquation\", 'accuracy: %.4f' % score_with_kfold(X, y, model_nor_equ))\n",
    "for iter in iter_list:\n",
    "    print(\"LinearRegression-iter %-4s\" %str(iter),\"accuracy: %.4f\" % score_with_kfold(X, y, LinearRegression, needIter=True, iter=iter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RidgeRegression(岭回归)\n",
    "为最小二乘法估计中加入一个扰动$\\lambda I$，使得原先无法求出广义逆的情况变成可以求出其广义逆，使得问题稳定并得以求解。同时，岭回归在计算`loss`时加入$L2$正则化，即:\n",
    "$$J = \\frac{1}{2m}\\sum_{i=1}^m(z^{(i)}-y^{(i)})^2 + \\lambda\\sum_{i=0}^nw_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression(LinearRegression):\n",
    "    def __init__(self, feature_dimension, lambdaI=0.01):\n",
    "        super().__init__(feature_dimension)\n",
    "        self.lambdaI = lambdaI\n",
    "    \n",
    "    def cal_loss(self, X, y):\n",
    "        \"\"\"calculate the loss function using input matrix X and vector y\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((sample_number, feature_dimension))): matrix X\n",
    "            y (numpy.array((feature_dimension, ))): vector y\n",
    "\n",
    "        Returns:\n",
    "            [float]: the loss of the training data\n",
    "            [numpy.array]: delta W\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        w = self.W\n",
    "        l = self.lambdaI\n",
    "        y_hat = np.dot(X, w)\n",
    "        dy = y_hat - y\n",
    "        loss = dy.T.dot(dy)\n",
    "        loss = loss / 2 / m\n",
    "        loss += l * np.sum(np.power(w, 2))\n",
    "        dw = X.T.dot(dy)/m\n",
    "        dw += 2 * l * w\n",
    "        return loss, dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeRegression-iter 0    accuracy: 0.3333\n",
      "RidgeRegression-iter 10   accuracy: 0.7400\n",
      "RidgeRegression-iter 100  accuracy: 0.9600\n",
      "RidgeRegression-iter 1000 accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "for iter in iter_list:\n",
    "    print(\"RidgeRegression-iter %-4s\"%str(iter),\"accuracy: %.4f\" % score_with_kfold(X, y, RidgeRegression, needIter=True, iter=iter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LassoRegression(Lasso回归)\n",
    "Lasso回归与岭回归类似，同样引入扰动$\\lambda I$，但在计算`loss`时加上$L1$正则化，也即:\n",
    "$$J = \\frac{1}{2m}\\sum_{i=1}^m(z^{(i)}-y^{(i)})^2 + \\lambda\\sum_{i=0}^n|w_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression(LinearRegression):\n",
    "    def __init__(self, feature_dimension, lambdaI=0.01):\n",
    "        super().__init__(feature_dimension)\n",
    "        self.lambdaI = lambdaI\n",
    "    \n",
    "    def cal_loss(self, X, y):\n",
    "        \"\"\"calculate the loss function using input matrix X and vector y\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((sample_number, feature_dimension))): matrix X\n",
    "            y (numpy.array((feature_dimension, ))): vector y\n",
    "\n",
    "        Returns:\n",
    "            [float]: the loss of the training data\n",
    "            [numpy.array]: delta W\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        w = self.W\n",
    "        l = self.lambdaI\n",
    "        y_hat = np.dot(X, w)\n",
    "        dy = y_hat - y\n",
    "        loss = dy.T.dot(dy)\n",
    "        loss = loss / 2 / m\n",
    "        loss += l * np.sum(np.abs(w))\n",
    "        dw = X.T.dot(dy)/m\n",
    "        dw += 2 * l * w\n",
    "        return loss, dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoRegression-iter 0    accuracy: 0.3333\n",
      "LassoRegression-iter 10   accuracy: 0.7400\n",
      "LassoRegression-iter 100  accuracy: 0.9600\n",
      "LassoRegression-iter 1000 accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "for iter in iter_list:\n",
    "    print(\"LassoRegression-iter %-4s\"%str(iter),\"accuracy: %.4f\" % score_with_kfold(X, y, LassoRegression, needIter=True, iter=iter))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso和Ridge的对比\n",
    "`RidgeRegression`中，以两个变量为例, 残差平方和可以表示为$w_1, w_2$的一个二次函数，是一个在三维空间中的抛物面，可以用等值线来表示。而限制条件$w_1^2+w_2^2<t$，相当于在二维平面的一个圆。这个时候等值线与圆相切的点便是在约束条件下的最优点。\n",
    "\n",
    "而对于`LassoRegression`而言，同样以两个变量为例，标准线性回归的$loss$还是可以用二维平面的等值线表示，而约束条件则与岭回归的圆不同，`lasso`的约束条件可以用方形表示。相比圆，方形的顶点更容易与抛物面相交，顶点就意味着对应的很多系数为0，而岭回归中的圆上的任意一点都很容易与抛物面相交很难得到正好等于0的系数。这也就意味着，`lasso`起到了很好的筛选变量的作用。\n",
    "\n",
    "![LassoRidge_Graphic](./LassoRidge.png \"`LassoRegression`(left) and `RidgeRegression`(right)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果分析和思考\n",
    "通过模型训练对标签进行预测，并在数据处理时采用了标准化的方式进行预处理。同时，通过正规方程法、梯度下降法和岭回归三种方法对模型进行训练，并得到了他们各自的预测准确率。还使用正规方程法和梯度下降法不同迭代次数对同一种花的数据进行了线性回归的可视化处理。\n",
    "\n",
    "下面，我将分析和思考各个处理的原因和特点。\n",
    "- 数据标准化\n",
    "\n",
    "    当一组数据有多个特征时，不同特征的数量级可能并不一样，各特征之间差距大会导致不同特征的权重不一样，从而使得结果不稳定、不可靠。因而可以通过将数据根据在自身维度上的大小统一映射到一个区间内，便消除了不同特征数量级的影响。\n",
    "\n",
    "    ![数据标准化](./normalization.jpeg)\n",
    "    \n",
    "\n",
    "- 正规方程法\n",
    "\n",
    "    正规方程法从均方差(MSE)出发，通过让均方差最小，从方程组出发，从数学角度解出参数矩阵$W$，得到数据可靠条件下的理论最优解。这种方式在数据可靠和噪点少的情况下可信度高，但\n",
    "  - 优点:\n",
    "\n",
    "  1. 在数据可靠和噪点少的情况下可信度高\n",
    "  - 缺点:\n",
    "\n",
    "  1. 模型的延展性较差，无法在已有模型的基础上进一步训练，只能通过重新建立方程组求解的方式来得到新的$W$矩阵。\n",
    "  2. 无法辨别出数据中的噪点，无法确定数据的权重，所有变量均被视为不同线性维度上的互不相关的变量，都拥有相同的权重。\n",
    "\n",
    "- 梯度下降法\n",
    "\n",
    "    梯度下降法也从均方差(MSE)出发，但并不直接解出$W$，而是通过梯度下降的方式，不断迭代逼近$W$，并且从实验结果可以看出，迭代次数足够多时，梯度下降法正确率与正规方程法相同，这是因为已经逼近到与理论最优解相等。\n",
    "  - 优点:\n",
    "\n",
    "  1. 模型延展性好，已有模型再加入数据训练也只需要增加迭代次数而对$W$进行调整。\n",
    "  - 缺点:\n",
    "\n",
    "  1. 本质上也是通过逼近正规方程结果得到，但当某些列间线性相关性大时，$X^TX$不满秩，接近奇异矩阵，误差很大。\n",
    "- 岭回归\n",
    "\n",
    "    岭回归是梯度下降法的一种优化解法，通过加上$\\lambda I$来对矩阵的秩进行调整，也就将不适定问题转化为了适定问题。\n",
    "\n",
    "  - 优点:\n",
    "\n",
    "  1. 降低了多重共线问题的出现概率，缓解了过拟合问题，增强了模型的稳定性和可靠性。\n",
    "  - 缺点:\n",
    "\n",
    "  1. $\\lambda I$的加入会损失部分信息、降低精度。\n",
    "  2. 没有从根本上解决多重共线问题。\n",
    "\n",
    "- Lasso回归\n",
    "\n",
    "    Lasso回归与岭回归类似，也是通过加上$\\lambda I$来对矩阵的秩进行调整，也就将不适定问题转化为了适定问题。\n",
    "  - 优点\n",
    "\n",
    "  1. 与岭回归类似缓解过拟合问题。\n",
    "  2. 比起岭回归更容易使部分权重变为0，从而可以进行`feature selection`\n",
    "  - 缺点\n",
    "  1. 会损失信息、降低精度，特别是进行`feature selection`。\n",
    "  2. 同样没有在根本上解决多重共线问题。\n",
    "  3. 无法得出显式解，智能使用近似化的计算法（坐标轴下降法和最小角回归法）估计出来的结果不太稳定，存在一定的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "本次实验我们分别通过正规方程法、梯度下降法和岭回归法三种方法对回归问题进行分析，证明了数学角度上的可行性。搭建了这三种模型，均在iris数据集上取得了良好的效果，并且三者在充分训练时(迭代次数足够时)，预测得到的正确率是相同的。这一过程中，我们通过标准化的方式消除了数据之间量级的差异，保证了数据的可靠性和模型的稳定性；利用kFold交叉验证算法保证了数据的充分利用和模型的可靠性。最终得到了约0.96的准确率。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "418a6164670b5d92bf37129a5a6635874e0387e34e155382adfe91ece8a749ee"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
