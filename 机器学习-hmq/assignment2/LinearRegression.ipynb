{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Machine Learning: Assignment 2</center>\n",
    "**<center>黄绵秋  19307130142</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 任务描述\n",
    "- 实现线性回归算法\n",
    "- 使用线性回归对iris数据集进行预测\n",
    "\n",
    "## 数据描述\n",
    "- iris数据集一共包含150个样本数据，按照iris种类分成3类，分别被标注为1,2,3，每个样本含有4个特征。\n",
    "- 将特征数据和标签数据分别从sklearn.dataset中导出到变量X,y中\n",
    "- iris数据集的一些信息如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 150\n",
      "feature_names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "target_names: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print('size:', len(X))\n",
    "print('feature_names:', iris.feature_names)\n",
    "print('target_names:', iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 数据处理\n",
    "- 不同特征的数值差别较大，这会导致数量级较大的属性占据主导地位，为防止数量级大的属性的少量变化引起收敛时的震荡和梯度爆炸，应对数据进行标准化处理。鉴于iris数据集较小较精确，且异常点和噪点极少，故采用归一化方式来保证各个特征相等的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[[1.         0.22222222 0.625      0.06779661 0.04166667]\n",
      " [1.         0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [1.         0.11111111 0.5        0.05084746 0.04166667]\n",
      " [1.         0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [1.         0.19444444 0.66666667 0.06779661 0.04166667]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:5])\n",
    "import sklearn.preprocessing\n",
    "X = sklearn.preprocessing.MinMaxScaler().fit_transform(X)\n",
    "X = np.pad(X, ((0,0),(1,0)), 'constant', constant_values=(1))       #为X填充x_0=1\n",
    "# x_0为常值1，用来与w_0相乘，表示偏移量bias\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为对回归结果进行有效评估，将数据分割成训练集和测试集两部分，其中70%为训练集，30%为测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.47222222 0.29166667 0.69491525 0.625     ]\n",
      " [1.         0.44444444 0.41666667 0.69491525 0.70833333]\n",
      " [1.         0.55555556 0.58333333 0.77966102 0.95833333]]\n",
      "数据集size:\n",
      " 训练集 (105, 5) (105,) \n",
      " 测试集 (45, 5) \t (45,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2021, shuffle=True)\n",
    "print(X_train[:3])\n",
    "print('数据集size:\\n', '训练集', X_train.shape, y_train.shape, '\\n 测试集', X_test.shape,'\\t', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归模型\n",
    "\n",
    "### 线性回归概述\n",
    "线性回归用于在监督学习中预测输入变量和输出变量之间的关系，输入变量的值变化时，预测结果也通过线性回归进行相应变化。回归模型正是表示从输入变量到输出变量之间映射的函数。\n",
    "\n",
    "### 算法推导\n",
    "#### 符号规定\n",
    "$x_j^{(i)}$表示数据集第$i$个数据的第$j$个属性取值，$y^{(i)}$表示数据集第$i$个数据的标签值，$z^{(i)}$表示第$i$组数据的计算预测值。数据集一共有$m$个数据，$n$种属性。\n",
    "\n",
    "矩阵形式表示，$x_i=\\begin{bmatrix}1 & x_1^{(i)} & \\cdots & x_n^{(i)}\\end{bmatrix}$，$X=\\begin{bmatrix}1 & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\ 1 & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ 1 & x_1^{(m)} & \\cdots & x_n^{(m)}\\end{bmatrix}$，参数$W=\\begin{bmatrix}w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n\\end{bmatrix}$，数据集标签为$y=\\begin{bmatrix}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)}\\end{bmatrix}$\n",
    "\n",
    "#### 模型目标\n",
    "模型定义为$f(x)=\\sum_{i=0}^n{w_ix_i}$，其中$x_0=1$，矩阵表示为$f(x)=XW$。我们的目的就是寻找最合适的$W$来最好效果地拟合和预测。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 正规方程法\n",
    "线性回归试图学得$z = \\sum_{i=0}^nw_ix_i$，使得$z\\simeq y$。而回归任务中常用的学习$W$手段便是均方差(MSE-mean squared error)：\n",
    "$$J=\\frac{1}{2m}\\sum_{i=1}^m(z_i-y_i)^2=\\frac{1}{2m}\\sum_{i=1}^m(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})^2$$\n",
    "我们要让均方差最小，也就要使得$\\frac{\\partial J}{\\partial w}$为0。以$w_k$为例\n",
    "$$\\frac{\\partial J}{\\partial w_k}=\\frac{\\partial( \\frac{1}{2m}\\sum_{i=1}^m(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})^2)}{\\partial w_k}=\\frac{1}{m}\\sum_{i=1}^m[(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})(-x_k^{(i)})]=0$$\n",
    "\n",
    "由此可以得到一个关于$w_k$的方程组，可由此解出各个$w_k$的值，方程组如下:\n",
    "$$\\begin{cases}\n",
    "\\sum_{i=1}^m[(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})(-x_0^{(i)})]=0\\\\\n",
    "\\sum_{i=1}^m[(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})(-x_1^{(i)})]=0\\\\\n",
    "\\cdots\\\\\n",
    "\\sum_{i=1}^m[(y^{(i)}-\\sum_{j=0}^nw_jx_j^{(i)})(-x_n^{(i)})]=0\n",
    "\\end{cases}$$\n",
    "化简如下:\n",
    "$$\\begin{cases}\n",
    "\\sum_{j=0}^n[w_jx_0^{(i)}(\\sum_{i=1}^mx_j^{(i)})]=\\sum_{i=1}^mx_0^{(i)}y^{(i)}\\\\\n",
    "\\sum_{j=0}^n[w_jx_1^{(i)}(\\sum_{i=1}^mx_j^{(i)})]=\\sum_{i=1}^mx_1^{(i)}y^{(i)}\\\\\n",
    "\\cdots\\\\\n",
    "\\sum_{j=0}^n[w_jx_n^{(i)}(\\sum_{i=1}^mx_j^{(i)})]=\\sum_{i=1}^mx_n^{(i)}y^{(i)}\n",
    "\\end{cases}$$\n",
    "\n",
    "我们要求解的便是向量$W=\\begin{bmatrix}w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n\\end{bmatrix}$。记矩阵$A=\\begin{bmatrix}\\sum_{i=1}^m(x_0^{(i)}x_0^{(i)}) & \\sum_{i=1}^m(x_0^{(i)}x_1^{(i)}) & \\cdots & \\sum_{i=1}^m(x_0^{(i)}x_n^{(i)}) \\\\ \\sum_{i=1}^m(x_1^{(i)}x_0^{(i)}) & \\sum_{i=1}^m(x_1^{(i)}x_1^{(i)}) & \\cdots & \\sum_{i=1}^m(x_1^{(i)}x_n^{(i)})\\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ \\sum_{i=1}^m(x_n^{(i)}x_0^{(i)}) & \\sum_{i=1}^m(x_n^{(i)}x_1^{(i)}) & \\cdots & \\sum_{i=1}^m(x_n^{(i)}x_n^{(i)}) \\end{bmatrix}$，矩阵$B=\\begin{bmatrix}\\sum_{i=1}^m(x_0^{(i)}y^{(i)})\\\\\\sum_{i=1}^m(x_1^{(i)}y^{(i)})\\\\\\cdots \\\\ \\sum_{i=1}^m(x_n^{(i)}y^{(i)})\\end{bmatrix}$。\n",
    "\n",
    "由线代知识易知\n",
    "$$AW=B \\\\ A^{T}AW=A^{T}B \\\\ W=(A^TA)^{-1}A^TB$$\n",
    "这个过程利用Python中解方程的函数即可求得W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降法\n",
    "与正规方程法类似，梯度下降法也是使得MSE尽可能小，但并不通过直接计算，而是根据梯度对数据进行调整，利用导数传递误差，再通过迭代的方式一步一步逼近真实解。\n",
    "\n",
    "损失函数$loss^{(i)}=\\frac{1}{2m}(z^{(i)}-y^{(i)})^2$\n",
    "\n",
    "计算$z$的梯度:$\\frac{\\partial J^{(i)}}{\\partial z^{(i)}}=\\frac{1}{m}(z^{(i)}-y^{(i)})$\n",
    "\n",
    "计算$W$的梯度:\n",
    "$$\\begin{aligned}\\frac{\\partial J}{\\partial W}&=\\begin{bmatrix}\\frac{\\partial J}{\\partial w_0}\\\\ \\frac{\\partial J}{\\partial w_1}\\\\ \\vdots\\\\ \\frac{\\partial J}{\\partial w_n}\\end{bmatrix}=\\begin{bmatrix}\\sum_{i=1}^m(\\frac{\\partial J}{\\partial z_i}\\frac{\\partial z_i}{\\partial w_0})\\\\\\sum_{i=1}^m(\\frac{\\partial J}{\\partial z_i}\\frac{\\partial z_i}{\\partial w_1})\\\\\\vdots\\\\\\sum_{i=1}^m(\\frac{\\partial J}{\\partial z_i}\\frac{\\partial z_i}{\\partial w_n})\\end{bmatrix} \\\\&=\\begin{bmatrix}\\frac{1}{m}\\sum_{i=1}^m(z^{(i)}-y^{(i)})x_0^{(i)}\\\\\\frac{1}{m}\\sum_{i=1}^m(z^{(i)}-y^{(i)})x_1^{(i)}\\\\\\vdots\\\\\\frac{1}{m}\\sum_{i=1}^m(z^{(i)}-y^{(i)})x_n^{(i)}\\end{bmatrix}\\\\&=\\frac{1}{m}\\begin{bmatrix}1 & x_1^{(1)} & \\cdots & x_n^{(1)} \\\\ 1 & x_1^{(2)} & \\cdots & x_n^{(2)} \\\\ \\cdots & \\cdots & \\cdots & \\cdots \\\\ 1 & x_1^{(m)} & \\cdots & x_n^{(m)}\\end{bmatrix}^T\\begin{bmatrix}z^{(1)}-y^{(1)}\\\\z^{(2)}-y^{(2)}\\\\\\vdots\\\\z^{(m)}-y^{(m)}\\end{bmatrix}\\\\&=\\frac{1}{m}X^T(Z-Y)\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型实现\n",
    "#### 最小二乘法实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalEquation():\n",
    "    def __init__(self, feature_dimension):\n",
    "        \"\"\"Initialize variable matrix W\n",
    "\n",
    "        Args:\n",
    "            feature_dimension (int): count of feature\n",
    "        \"\"\"\n",
    "        self.D = feature_dimension\n",
    "        self.W = np.zeros((self.D, ))\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Use normal equation with input X and y to calculate W\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((sample_number, feature_dimension))): matrix X\n",
    "            y (numpy.array((feature_dimension, ))): vector y\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        n = X.shape[1]\n",
    "        A = [[np.sum([X[i][k]*X[i][j] for i in range(m)]) for k in range(n)] for j in range(n)]\n",
    "        A = np.array(A)\n",
    "        B = [np.sum([X[i][j]*y[i] for i in range(m)]) for j in range(n)]\n",
    "        B = np.array(B).T\n",
    "        self.W = np.linalg.solve(A, B)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Use variable matrix W to predict the label result\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((test_sample, feature_dimension))): the input matrix X\n",
    "\n",
    "        Returns:\n",
    "            [int]: the prediction of the label result.\n",
    "        \"\"\"\n",
    "        m = np.array(np.dot(X, self.W))\n",
    "        return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度下降法实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self, feature_dimension):\n",
    "        \"\"\"Initialize variable matrix W\n",
    "\n",
    "        Args:\n",
    "            feature_dimension (int): count of feature\n",
    "        \"\"\"\n",
    "        self.D = feature_dimension\n",
    "        self.W = np.zeros((self.D, ))\n",
    "        return\n",
    "\n",
    "    def cal_loss(self, X, y):\n",
    "        \"\"\"calculate the loss function using input matrix X and vector y\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((sample_number, feature_dimension))): matrix X\n",
    "            y (numpy.array((feature_dimension, ))): vector y\n",
    "\n",
    "        Returns:\n",
    "            [float]: the loss of the training data\n",
    "            [numpy.array]: delta W\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        w = self.W\n",
    "        y_hat = np.dot(X, w)\n",
    "        dy = y_hat - y\n",
    "        loss = dy.T.dot(dy)\n",
    "        loss = loss / 2 / m\n",
    "        dw = X.T.dot(dy)/m\n",
    "        return loss, dw\n",
    "        \n",
    "    def train(self, X, y, learning_rate=0.99, iteration=1000, loss_print=False):\n",
    "        \"\"\"Use input matrix X and vector y to train the model.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((sample_number, feature_dimension))): matrix X\n",
    "            y (numpy.array((feature_dimension, ))): vector y\n",
    "            learning_rate (float, optional): learning rate(may also called alpha). Defaults to 0.99.\n",
    "            iteration (int, optional): Number of iterations. Defaults to 1000.\n",
    "            loss_print (bool, optional): True if want to print the loss. Defaults to False.\n",
    "        \"\"\"\n",
    "        loss, dw = None, None\n",
    "        for i in range(iteration):\n",
    "            loss, dw = self.cal_loss(X, y)\n",
    "            self.W = self.W - learning_rate * dw\n",
    "            if loss_print:\n",
    "                print('Iteration: %d , Loss: %.2f' % (i, loss))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Use variable matriX W to predict the label result\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((test_sample, feature_dimension))): the input matrix X\n",
    "\n",
    "        Returns:\n",
    "            [int]: the prediction of the label result.\n",
    "        \"\"\"\n",
    "        return X.dot(self.W)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型结果\n",
    "#### 预测正确性判断\n",
    "分别用两种模型进行训练并预测，并取四舍五入的结果作为预测结果，通过统计预测的正确性来判断模型的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal equation test score: 0.9556\n",
      "Linear regression with 10 iterations test score: 0.7333\n",
      "Linear regression with 100 iterations test score: 0.9556\n",
      "Linear regression with 1000 iterations test score: 0.9556\n",
      "Linear regression with 10000 iterations test score: 0.9556\n"
     ]
    }
   ],
   "source": [
    "def score(test_result, y_test):\n",
    "    counter = 0\n",
    "    acc = 0\n",
    "    for (pred_label, true_label) in zip(test_result, y_test):\n",
    "        counter += 1\n",
    "        if pred_label == true_label:\n",
    "            acc += 1\n",
    "    return acc/counter\n",
    "\n",
    "    \n",
    "normal_equation = NormalEquation(X_train.shape[1])\n",
    "normal_equation.train(X_train, y_train)\n",
    "test_result = normal_equation.predict(X_test).round()\n",
    "print('Normal equation test score: %.4f' % score(test_result, y_test))\n",
    "\n",
    "lr = LinearRegression(X_train.shape[1])\n",
    "lr.train(X_train, y_train, iteration=10)\n",
    "test_result = lr.predict(X_test).round()\n",
    "print('Linear regression with 10 iterations test score: %.4f' % score(test_result, y_test))\n",
    "lr.train(X_train, y_train, iteration=100)\n",
    "test_result = lr.predict(X_test).round()\n",
    "print('Linear regression with 100 iterations test score: %.4f' % score(test_result, y_test))\n",
    "lr.train(X_train, y_train, iteration=1000)\n",
    "test_result = lr.predict(X_test).round()\n",
    "print('Linear regression with 1000 iterations test score: %.4f' % score(test_result, y_test))\n",
    "lr.train(X_train, y_train, iteration=10000)\n",
    "test_result = lr.predict(X_test).round()\n",
    "print('Linear regression with 10000 iterations test score: %.4f' % score(test_result, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性回归绘制\n",
    "取一个特征维度，分别用两种模型进行线性回归，并将回归的直线进行可视化处理，以此直观地判断拟合的程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "regression_X = iris.data[:50]\n",
    "regression_X = sklearn.preprocessing.MinMaxScaler().fit_transform(regression_X)\n",
    "regression_X = np.pad(regression_X, ((0,0),(1,0)), 'constant', constant_values=(1))       #为X填充x_0=1\n",
    "regression_y = iris.target[:50]\n",
    "reg_X_train, reg_X_test, reg_y_train, reg_y_test = train_test_split(regression_X, regression_y, test_size=0.3, random_state=2021, shuffle=True)\n",
    "\n",
    "show_X_train = reg_X_train[:,:2]\n",
    "show_X_test = reg_X_test[:,:2]\n",
    "show_X_test_scatter = reg_X_test[:,1]\n",
    "show_y_train = reg_X_train[:,2:3]\n",
    "show_y_test = reg_X_test[:,2:3]\n",
    "show_y_train = show_y_train.reshape((show_y_train.shape[0],))\n",
    "show_y_test = show_y_test.reshape((show_y_test.shape[0],))\n",
    "\n",
    "show_nor_equ = NormalEquation(2)\n",
    "show_nor_equ.train(show_X_train, show_y_train)\n",
    "show_lr_0 = LinearRegression(2)\n",
    "show_lr_0.train(show_X_train, show_y_train, iteration=0)\n",
    "show_lr_10 = LinearRegression(2)\n",
    "show_lr_10.train(show_X_train, show_y_train, iteration=10)\n",
    "show_lr_100 = LinearRegression(2)\n",
    "show_lr_100.train(show_X_train, show_y_train, iteration=100)\n",
    "show_lr_1000 = LinearRegression(2)\n",
    "show_lr_1000.train(show_X_train, show_y_train, iteration=1000)\n",
    "\n",
    "model_dict = {\n",
    "    show_nor_equ: \"NormalEquation\",\n",
    "    show_lr_0: \"LinearRegression: iter-0\",\n",
    "    show_lr_10: \"LinearRegression: iter-10\",\n",
    "    show_lr_100: \"LinearRegression: iter-100\",\n",
    "    show_lr_1000: \"LinearRegression: iter-1000\"\n",
    "}\n",
    "plt.xlabel('x label')\n",
    "plt.ylabel('y label')\n",
    "plt.scatter(show_X_test_scatter, show_y_test)\n",
    "for model, model_str in model_dict.items():\n",
    "    # model.train(show_X_train, show_y_train)\n",
    "    plt.plot(show_X_test_scatter, model.predict(show_X_test), label=model_str)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型优化\n",
    "#### kFold算法交叉验证\n",
    "将数据集k等分进行交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def score_with_kfold(X, y, model, needIter=False, iter=1000):\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=2021)\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "        Model = model(X_train.shape[1])\n",
    "        if needIter == False:\n",
    "            Model.train(X_train, y_train)\n",
    "        else:\n",
    "            Model.train(X_train, y_train, iteration=iter)\n",
    "        test_result = Model.predict(X_test).round()\n",
    "        scores.append(score(test_result, y_test))\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NormalEquation accuracy: 0.9600\n",
      "LinearRegression-iter 0    accuracy: 0.3333\n",
      "LinearRegression-iter 10   accuracy: 0.7933\n",
      "LinearRegression-iter 100  accuracy: 0.9600\n",
      "LinearRegression-iter 1000 accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "X = sklearn.preprocessing.MinMaxScaler().fit_transform(X)\n",
    "X = np.pad(X, ((0,0),(1,0)), 'constant', constant_values=(1))       #为X填充x_0=1\n",
    "model_nor_equ = NormalEquation\n",
    "iter_list = [0, 10, 100, 1000]\n",
    "print(\"NormalEquation\", 'accuracy: %.4f' % score_with_kfold(X, y, model_nor_equ))\n",
    "for iter in iter_list:\n",
    "    print(\"LinearRegression-iter %-4s\" %str(iter),\"accuracy: %.4f\" % score_with_kfold(X, y, LinearRegression, needIter=True, iter=iter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RidgeRegression(岭回归)\n",
    "为最小二乘法估计中加入一个扰动$\\lambda I$，使得原先无法求出广义逆的情况变成可以求出其广义逆，使得问题稳定并得以求解。同时，岭回归在计算`loss`时加入$L2$正则化，即:\n",
    "$$J = \\frac{1}{2m}\\sum_{i=1}^m(z^{(i)}-y^{(i)})^2 + \\lambda\\sum_{i=0}^nw_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression(LinearRegression):\n",
    "    def __init__(self, feature_dimension, lambdaI=0.01):\n",
    "        super().__init__(feature_dimension)\n",
    "        self.lambdaI = lambdaI\n",
    "    \n",
    "    def cal_loss(self, X, y):\n",
    "        \"\"\"calculate the loss function using input matrix X and vector y\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((sample_number, feature_dimension))): matrix X\n",
    "            y (numpy.array((feature_dimension, ))): vector y\n",
    "\n",
    "        Returns:\n",
    "            [float]: the loss of the training data\n",
    "            [numpy.array]: delta W\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        w = self.W\n",
    "        l = self.lambdaI\n",
    "        y_hat = np.dot(X, w)\n",
    "        dy = y_hat - y\n",
    "        loss = dy.T.dot(dy)\n",
    "        loss = loss / 2 / m\n",
    "        loss += l * np.sum(np.power(w, 2))\n",
    "        dw = X.T.dot(dy)/m\n",
    "        dw += 2 * l * w\n",
    "        return loss, dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeRegression-iter 0    accuracy: 0.3333\n",
      "RidgeRegression-iter 10   accuracy: 0.7400\n",
      "RidgeRegression-iter 100  accuracy: 0.9600\n",
      "RidgeRegression-iter 1000 accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "for iter in iter_list:\n",
    "    print(\"RidgeRegression-iter %-4s\"%str(iter),\"accuracy: %.4f\" % score_with_kfold(X, y, RidgeRegression, needIter=True, iter=iter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LassoRegression(Lasso回归)\n",
    "Lasso回归与岭回归类似，同样引入扰动$\\lambda I$，但在计算`loss`时加上$L1$正则化，也即:\n",
    "$$J = \\frac{1}{2m}\\sum_{i=1}^m(z^{(i)}-y^{(i)})^2 + \\lambda\\sum_{i=0}^n|w_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression(LinearRegression):\n",
    "    def __init__(self, feature_dimension, lambdaI=0.01):\n",
    "        super().__init__(feature_dimension)\n",
    "        self.lambdaI = lambdaI\n",
    "    \n",
    "    def cal_loss(self, X, y):\n",
    "        \"\"\"calculate the loss function using input matrix X and vector y\n",
    "\n",
    "        Args:\n",
    "            X (numpy.array((sample_number, feature_dimension))): matrix X\n",
    "            y (numpy.array((feature_dimension, ))): vector y\n",
    "\n",
    "        Returns:\n",
    "            [float]: the loss of the training data\n",
    "            [numpy.array]: delta W\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        w = self.W\n",
    "        l = self.lambdaI\n",
    "        y_hat = np.dot(X, w)\n",
    "        dy = y_hat - y\n",
    "        loss = dy.T.dot(dy)\n",
    "        loss = loss / 2 / m\n",
    "        loss += l * np.sum(np.abs(w))\n",
    "        dw = X.T.dot(dy)/m\n",
    "        dw += 2 * l * w\n",
    "        return loss, dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoRegression-iter 0    accuracy: 0.3333\n",
      "LassoRegression-iter 10   accuracy: 0.7400\n",
      "LassoRegression-iter 100  accuracy: 0.9600\n",
      "LassoRegression-iter 1000 accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "for iter in iter_list:\n",
    "    print(\"LassoRegression-iter %-4s\"%str(iter),\"accuracy: %.4f\" % score_with_kfold(X, y, LassoRegression, needIter=True, iter=iter))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso和Ridge的对比\n",
    "`RidgeRegression`中，以两个变量为例, 残差平方和可以表示为$w_1, w_2$的一个二次函数，是一个在三维空间中的抛物面，可以用等值线来表示。而限制条件$w_1^2+w_2^2<t$，相当于在二维平面的一个圆。这个时候等值线与圆相切的点便是在约束条件下的最优点。\n",
    "\n",
    "而对于`LassoRegression`而言，同样以两个变量为例，标准线性回归的$loss$还是可以用二维平面的等值线表示，而约束条件则与岭回归的圆不同，`lasso`的约束条件可以用方形表示。相比圆，方形的顶点更容易与抛物面相交，顶点就意味着对应的很多系数为0，而岭回归中的圆上的任意一点都很容易与抛物面相交很难得到正好等于0的系数。这也就意味着，`lasso`起到了很好的筛选变量的作用。\n",
    "\n",
    "![LassoRidge_Graphic](./LassoRidge.png \"`LassoRegression`(left) and `RidgeRegression`(right)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果分析和思考\n",
    "通过模型训练对标签进行预测，并在数据处理时采用了标准化的方式进行预处理。同时，通过正规方程法、梯度下降法和岭回归三种方法对模型进行训练，并得到了他们各自的预测准确率。还使用正规方程法和梯度下降法不同迭代次数对同一种花的数据进行了线性回归的可视化处理。\n",
    "\n",
    "下面，我将分析和思考各个处理的原因和特点。\n",
    "- 数据标准化\n",
    "\n",
    "    当一组数据有多个特征时，不同特征的数量级可能并不一样，各特征之间差距大会导致不同特征的权重不一样，从而使得结果不稳定、不可靠。因而可以通过将数据根据在自身维度上的大小统一映射到一个区间内，便消除了不同特征数量级的影响。\n",
    "\n",
    "    ![数据标准化](./normalization.jpeg)\n",
    "    \n",
    "\n",
    "- 正规方程法\n",
    "\n",
    "    正规方程法从均方差(MSE)出发，通过让均方差最小，从方程组出发，从数学角度解出参数矩阵$W$，得到数据可靠条件下的理论最优解。这种方式在数据可靠和噪点少的情况下可信度高，但\n",
    "  - 优点:\n",
    "\n",
    "  1. 在数据可靠和噪点少的情况下可信度高\n",
    "  - 缺点:\n",
    "\n",
    "  1. 模型的延展性较差，无法在已有模型的基础上进一步训练，只能通过重新建立方程组求解的方式来得到新的$W$矩阵。\n",
    "  2. 无法辨别出数据中的噪点，无法确定数据的权重，所有变量均被视为不同线性维度上的互不相关的变量，都拥有相同的权重。\n",
    "\n",
    "- 梯度下降法\n",
    "\n",
    "    梯度下降法也从均方差(MSE)出发，但并不直接解出$W$，而是通过梯度下降的方式，不断迭代逼近$W$，并且从实验结果可以看出，迭代次数足够多时，梯度下降法正确率与正规方程法相同，这是因为已经逼近到与理论最优解相等。\n",
    "  - 优点:\n",
    "\n",
    "  1. 模型延展性好，已有模型再加入数据训练也只需要增加迭代次数而对$W$进行调整。\n",
    "  - 缺点:\n",
    "\n",
    "  1. 本质上也是通过逼近正规方程结果得到，但当某些列间线性相关性大时，$X^TX$不满秩，接近奇异矩阵，误差很大。\n",
    "- 岭回归\n",
    "\n",
    "    岭回归是梯度下降法的一种优化解法，通过加上$\\lambda I$来对矩阵的秩进行调整，也就将不适定问题转化为了适定问题。\n",
    "\n",
    "  - 优点:\n",
    "\n",
    "  1. 降低了多重共线问题的出现概率，缓解了过拟合问题，增强了模型的稳定性和可靠性。\n",
    "  - 缺点:\n",
    "\n",
    "  1. $\\lambda I$的加入会损失部分信息、降低精度。\n",
    "  2. 没有从根本上解决多重共线问题。\n",
    "\n",
    "- Lasso回归\n",
    "\n",
    "    Lasso回归与岭回归类似，也是通过加上$\\lambda I$来对矩阵的秩进行调整，也就将不适定问题转化为了适定问题。\n",
    "  - 优点\n",
    "\n",
    "  1. 与岭回归类似缓解过拟合问题。\n",
    "  2. 比起岭回归更容易使部分权重变为0，从而可以进行`feature selection`\n",
    "  - 缺点\n",
    "  1. 会损失信息、降低精度，特别是进行`feature selection`。\n",
    "  2. 同样没有在根本上解决多重共线问题。\n",
    "  3. 无法得出显式解，智能使用近似化的计算法（坐标轴下降法和最小角回归法）估计出来的结果不太稳定，存在一定的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "本次实验我们分别通过正规方程法、梯度下降法和岭回归法三种方法对回归问题进行分析，证明了数学角度上的可行性。搭建了这三种模型，均在iris数据集上取得了良好的效果，并且三者在充分训练时(迭代次数足够时)，预测得到的正确率是相同的。这一过程中，我们通过标准化的方式消除了数据之间量级的差异，保证了数据的可靠性和模型的稳定性；利用kFold交叉验证算法保证了数据的充分利用和模型的可靠性。最终得到了约0.96的准确率。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "16f5b46f222e2a3e8d4adbf7141cae37b71ed37616e60735fa5d1164a1bc3ada"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
